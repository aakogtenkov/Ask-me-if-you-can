{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Лабораторная №4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSA\n",
    "\n",
    "Прежде всего считаем файл, полученный в результате парсинга. Запишем в словарь все слова.\n",
    "\"tttrrr\" - это пометка, что началась новая статья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413444 37810\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "input = open('simplewiki-parsed3.txt', 'r')\n",
    "word_dict = dict()\n",
    "now_context = 0\n",
    "s = input.readline()\n",
    "c = 0\n",
    "prev_c = 0\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    if len(s) > 0 and s[0] == \"tttrrr\":\n",
    "        s = []\n",
    "        now_context += 1\n",
    "    for elem in s:\n",
    "        if not(elem in word_dict):\n",
    "            wl = len(word_dict)\n",
    "            word_dict[elem] = wl\n",
    "    s = input.readline()\n",
    "    c += 1\n",
    "    if c // 26000 > prev_c:\n",
    "        prev_c += 1\n",
    "        #print(prev_c)\n",
    "print(now_context, len(word_dict))\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Создадим разреженную матрицу. Запишем в нее стартовые значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37810, 413446)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "mas = lil_matrix((len(word_dict), now_context + 2), dtype='float16')\n",
    "print(mas.shape)\n",
    "print(mas[2473, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "413444 37810\n"
     ]
    }
   ],
   "source": [
    "input = open('simplewiki-parsed3.txt', 'r')\n",
    "now_context = 0\n",
    "s = input.readline()\n",
    "c = 0\n",
    "prev_c = 0\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    if len(s) > 0 and s[0] == \"tttrrr\":\n",
    "        s = []\n",
    "        now_context += 1\n",
    "    for elem in s:\n",
    "        mas[word_dict[elem], now_context] += 1\n",
    "    s = input.readline()\n",
    "    c += 1\n",
    "    if c // 26000 > prev_c:\n",
    "        prev_c += 1\n",
    "        #print(prev_c)\n",
    "print(now_context, len(word_dict))\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Запустим SVD-разложение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components = 128, n_iter = 8)\n",
    "mas = svd.fit_transform(mas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Сохраним результат. Матрица $mas$ - это матрица, в которой строки соответствуют словам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numpy.save('mas-1', mas)\n",
    "output = open('words-1.txt', 'w')\n",
    "for elem in word_dict:\n",
    "    output.write(elem + ' ' + str(word_dict[elem]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Это код для считывания $mas$ и списка слов из файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37810, 128)\n",
      "413444 37810\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import copy\n",
    "\n",
    "mas = numpy.load('mas-1.npy')\n",
    "print(mas.shape)\n",
    "\n",
    "input = open('words-1.txt', 'r')\n",
    "word_dict = dict()\n",
    "s = input.readline()\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    num = int(s[1])\n",
    "    word_dict[s[0]] = num\n",
    "    s = input.readline()\n",
    "print(now_context, len(word_dict))\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Функции расстояния: обычное евклидова мера, косинусная мера и измененная мера Минковского (мера Минковского подразумевало бы возведение в слишком большую степень, что приводило бы к большой погрешности)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def get_dist(word1, word2, k = 128):\n",
    "    ind1 = word_dict[word1]\n",
    "    ind2 = word_dict[word2]\n",
    "    d = numpy.sum((mas[ind1] - mas[ind2]) * (mas[ind1] - mas[ind2]))\n",
    "    return d\n",
    "\n",
    "def get_dist_v(vect, word2, k = 128):\n",
    "    vect2 = mas[word_dict[word2]]\n",
    "    d = numpy.sum((vect2 - vect) * (vect2 - vect))\n",
    "    return d\n",
    "\n",
    "def get_cos_dist(word1, word2, k = 128):\n",
    "    ind1 = word_dict[word1]\n",
    "    ind2 = word_dict[word2]\n",
    "    dot_prod = numpy.dot(mas[ind1], mas[ind2])\n",
    "    d1 = sqrt(numpy.dot(mas[ind1], mas[ind1]))\n",
    "    d2 = sqrt(numpy.dot(mas[ind2], mas[ind2]))\n",
    "    return (dot_prod / (d1 * d2))\n",
    "\n",
    "def get_cos_dist_v(vect, word2, k = 128):\n",
    "    ind2 = word_dict[word2]\n",
    "    dot_prod = numpy.dot(vect, mas[ind2])\n",
    "    d1 = sqrt(numpy.dot(vect, vect))\n",
    "    d2 = sqrt(numpy.dot(mas[ind2], mas[ind2]))\n",
    "    return (dot_prod / (d1 * d2))\n",
    "\n",
    "def get_half_minkowski_dist(word1, word2, k = 128, p = 8):\n",
    "    ind1 = word_dict[word1]\n",
    "    ind2 = word_dict[word2]\n",
    "    d = 0\n",
    "    for i in range(k):\n",
    "        d += (mas[ind1][i] - mas[ind2][i])**p\n",
    "    d **= (1/p)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В приведенном ниже коде можно тремя разными способами получать значение расстояния, и код будет находить ближайшие 10 слов (в порядке убывания близости)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_nearest(string = \"thrones\", n = 11, metric = \"cos\"):\n",
    "    min_d = [10000000000000000000000000 for i in range(n)]\n",
    "    ans = ['' for i in range(len(min_d))]\n",
    "    for word in word_dict:\n",
    "        if metric == \"euclid\":\n",
    "            d = get_dist(string, word)\n",
    "        elif metric == \"cos\":\n",
    "            d = 2 - get_cos_dist(string, word)\n",
    "        else:\n",
    "            d = get_half_minkowski_dist(string, word)\n",
    "        pst = False\n",
    "        for p in range(0, len(min_d)):\n",
    "            if not(pst) and d < min_d[p]:\n",
    "                for j in range(len(min_d) - 1, p, -1):\n",
    "                    min_d[j] = min_d[j - 1]\n",
    "                    ans[j] = ans[j - 1]\n",
    "                min_d[p] = d\n",
    "                ans[p] = word\n",
    "                pst = True\n",
    "    print(string + \": [\" + ', '.join(ans) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor: [actor, comedian, producer, actress, animator, cartoonist, screenwriter, journalist, director, boxer, activist]\n"
     ]
    }
   ],
   "source": [
    "get_nearest('actor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ниже я привожу некоторые слова и выданные программой списки близких слов (согласно заданию, нужно было выбрать 30 слов, но я выбрал около 25, ибо не хватило фантазии. 30 слов приведены для skip-gram модели).\n",
    "\n",
    "### Евклидова метрика:\n",
    "\n",
    "putin: [putin, abdel, merkel, dalai, fidel, abkhazia, sarkozy, ossetia, aliyev, goh]\n",
    "\n",
    "moscow: [moscow, vladimir, liberation, ukraine, sicily, milan, munich, prussia, prague, constantine]\n",
    "\n",
    "lord: [lord, rome, sons, palace, christ, lady, jerusalem, throne, heaven, cross, temple]\n",
    "\n",
    "computer: [computer, particular, software, web, standard, happens, systems, warnings, itself, normal, separate]\n",
    "\n",
    "siberia: [siberia, leased, russia's, tripoli, stormed, airspace, phan, wooded, xian, surabaya, muhammed]       (казалось бы, при чем тут Триполи??)\n",
    "\n",
    "morning: [morning, evening, afternoon, midnight, stayed, arrived, dropped, trip, deep, broke, murder]\n",
    "\n",
    "moon: [moon, planet, deep, sky, secret, rise, beauty, sun, journey, dog, 20th]\n",
    "\n",
    "nuclear: [nuclear, world's, economic, economy, growth, security, iraq, islamic, afghanistan, industrial, billion]\n",
    "\n",
    "ship: [ship, officers, battles, remained, orders, boat, fired, surrender, destroyed, destroy, armed]\n",
    "\n",
    "job: [job, stay, bring, deal, complete, rest, lots, accept, thinking, trouble, moment]\n",
    "\n",
    "escape: [escape, turns, alive, trapped, falling, stolen, burning, sword, steal, revenge, thrown]\n",
    "\n",
    "wikipedia: [wikipedia, please, changes, times, about, palatino, message, been, signpost, your, more]\n",
    "\n",
    "outpost: [outpost, dusk, cheering, bombard, receipt, urging, deakin, sloop, steamed, foreground, deed]\n",
    "\n",
    "car: [car, market, thames, freight, spa, self, express, cars, f1, races, driver]\n",
    "\n",
    "team: [team, won, championship, wmf, foundation, future, win, notifications, across, players, teams]\n",
    "\n",
    "\n",
    "hobbit: [hobbit, elves, gollum, dwarves, bilbo, orcs, sauron, hobbits, tolkien, pippin, baggins]\n",
    "\n",
    "forest: [forest, wood, garden, counties, yorkshire, gardens, oak, greater, salt, palm, zone]\n",
    "\n",
    "dragon: [dragon, sword, monster, lion, quest, spy, hercules, heroes, chase, wears, dreams]\n",
    "\n",
    "imperial: [imperial, arrived, fleet, supplies, occupied, losses, commanded, supply, landed, officials, invaded]\n",
    "\n",
    "metric: [metric, measurement, martinvl, defacto, centaur, indicated, metrication, measures, templatedata, nuts, attribute]\n",
    "\n",
    "empty: [empty, blank, specify, parent, specified, setting, replace, maintenance, defined, window, settings]\n",
    "\n",
    "dinosaurs: [dinosaurs, fossils, cretaceous, evolved, fossil, feathers, skull, bones, reptiles, extinction, jurassic]\n",
    "\n",
    "actor: [actor, actress, singer, director, player, former, writer, politician, musician, author, composer]        (есть ощущение, что не хватило данных, чтобы программа научилась как следует различать людей творческих профессий)\n",
    "\n",
    "light: [light, energy, sun, inside, earth, nature, side, dark, theory, surface, lower]\n",
    "\n",
    "jedi: [jedi, sith, unstoppable, puppies, wrath, toon, snowman, smurfs, dante's, bug's, looney]\n",
    "\n",
    "september: [september, 20, 21, 12, 26, 23, 30, block, 19, 24, i'm]             (мораль - убирайте даты из текста)\n",
    "\n",
    "\n",
    "\n",
    "### Косинусная метрика:\n",
    "\n",
    "putin: [putin, haider, gaza, medvedev, merkel, al-assad, bashar, abdel, jong-un, president, modi]\n",
    "\n",
    "moscow: [moscow, ii, basel, yuri, russia, vasili, bratislava, genoa, cologne, lugano, leningrad]\n",
    "\n",
    "lord: [lord, knights, king, sons, honourable, son, elder, jerusalem, god, vere, voyage]\n",
    "\n",
    "computer: [computer, laptop, processing, minimize, straightforward, computers, reciprocal, fizz, advantages, 4px, used]\n",
    "\n",
    "siberia: [siberia, phan, scandinavian, russia's, balkan, leased, vladivostok, thessaloniki, bight, xinjiang, tripoli]\n",
    "\n",
    "morning: [morning, evening, midnight, reached, afternoon, ten, stopped, six, along, later, late]\n",
    "\n",
    "moon: [moon, sun, planet, dust, deep, dark, story, journey, sky, secret, earth]\n",
    "\n",
    "nuclear: [nuclear, reactors, economic, uranium, industrial, world's, environmental, islamic, trade, reactor, prices]\n",
    "\n",
    "ship: [ship, orders, destroyed, fired, ship's, officers, battles, remained, shot, stationed, boat]\n",
    "\n",
    "job: [job, very, time, come, go, stay, much, become, well, take, thought]\n",
    "\n",
    "wikipedia: [wikipedia, recent, wikipedia's, read, report, about, been, changes, message, arbitration, reports]\n",
    "\n",
    "escape: [escape, kill, fight, escaping, thrown, sacrifice, trapped, crushed, fate, steal, turns]\n",
    "\n",
    "outpost: [outpost, stationed, remained, retreating, retreat, barges, capturing, resulted, captured, battles, command]\n",
    "\n",
    "car: [car, 2001, cars, interior, spa, market, loaned, virgin, jaguar, champ, cafe]\n",
    "\n",
    "team: [team, champions, winning, win, tournament, arena, won, matches, compete, brands, scored]\n",
    "\n",
    "hobbit: [hobbit, elves, gollum, sauron, bilbo, dwarves, middle-earth, frodo, orcs, gandalf, baggins]\n",
    "\n",
    "forest: [forest, gippsland, valley, central, wood, pier, enfield, dover, north, green, gardens]\n",
    "\n",
    "dragon: [dragon, revenge, madness, hercules, sword, magical, frees, disguised, ghoul, monster, ghosts]\n",
    "\n",
    "imperial: [imperial, emperor's, raids, fled, morale, invade, emperor, arrived, invasion, fleet, jing]\n",
    "\n",
    "metric: [metric, measurement, metrication, martinvl, bipm, kilogram, centaur, defacto, customary, indentured, thrasymedes]\n",
    "\n",
    "empty: [empty, invalid, corresponding, specify, set, duplicate, template, syntax, inserted, template's, text]\n",
    "\n",
    "dinosaurs: [dinosaurs, theropods, cretaceous, archosaurs, bipedal, archaeopteryx, fossils, mesozoic, theropod, fossil, dinosauria]\n",
    "\n",
    "actor: [actor, comedian, producer, actress, animator, cartoonist, screenwriter, journalist, director, boxer, activist]\n",
    "\n",
    "light: [light, bright, waves, smooth, glass, hole, pair, energy, sun, brightness, narrow]\n",
    "\n",
    "jedi: [jedi, alien, wrath, adventure, sith, lego, ninja, voices, toon, phantom, aliens]\n",
    "\n",
    "\n",
    "### Измененная метрика Минковского:\n",
    "\n",
    "moon: [moon, planet, deep, magic, secret, yellow, opening, beautiful, volume, production, unknown]\n",
    "\n",
    "job: [job, bring, stay, deal, rest, complete, completely, quickly, moment, thinking, decided]\n",
    "\n",
    "wikipedia: [wikipedia, your, palatino, times, changes, message, wikidata, please, been, signpost, make]\n",
    "\n",
    "outpost: [outpost, dusk, urging, calais, deserted, foreground, resisting, anchored, foe, cheering, deed]\n",
    "\n",
    "car: [car, wins, driver, belgian, brazilian, cars, races, lotus, motor, hungarian, bull]\n",
    "\n",
    "team: [team, won, previous, form, foundation, across, both, wmf, provide, four, end]\n",
    "\n",
    "hobbit: [hobbit, elves, gollum, pippin, bilbo, sauron, orcs, springtime, cauldron, minerva, figaro]\n",
    "\n",
    "forest: [forest, wood, garden, mars, greater, adams, zone, counties, banks, northwest, wolf]\n",
    "\n",
    "dragon: [dragon, sword, monster, quest, chase, revealed, spy, magic, heroes, lion, journey]\n",
    "\n",
    "imperial: [imperial, fleet, battles, invasion, launched, sons, refused, victory, headquarters, rising, officials]\n",
    "\n",
    "metric: [metric, measurement, centaur, define, indicated, root, indicates, relation, martinvl, nervous, grounds]\n",
    "\n",
    "empty: [empty, belong, authors, paste, replace, applies, contain, heading, titles, parent, translated]\n",
    "\n",
    "dinosaurs: [dinosaurs, fossil, cretaceous, fossils, evolved, reptiles, jurassic, extinction, snakes, feathers, dinosaur]\n",
    "\n",
    "light: [light, energy, earth, front, together, nature, ancient, outside, inside, lines, theory]\n",
    "\n",
    "\n",
    "\n",
    "Несложно видеть, что в среднем косинусная мера выдает самый ожидаемый результат, хотя иногда ее опережает евклидова мера (особенно это видно при входном слове 'moscow'). Измененная мера Минковского, на мой взгляд, несмотря на кажущуюся ущербность, выдает иногда довольно нетривиальные, не совсем близкие, но все же связанные с исходным словом слова. Возможно, с помощью этой меры можно немного абстрагироваться от исходного слова. Например, тогда как косинусная мера связывала со словом 'dinosaurs' всякие виды динозавров, измененная мера Минковского среди близких слов нашла слово 'snake'. Мне кажется, это вполне неплох, если нужно уметь выйти за рамки данного контекста и подумать немного о более широких вещах. Конечно, в других случаях измененная мера Минковского дает какие-то странные ответы, но тем не менее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Skip-gram\n",
    "\n",
    "Модель Skip-gram можно представить так: [входной вектор] -> [матрица представлений слов W$\\cdot$d] -> [матрица представлений контекстов d$\\cdot$W] -> [выходной вектор].\n",
    "\n",
    "В качестве входного вектора будем подавать вектор, где все нули, а на месте, соответствующем слову, стоит 1; в качестве выходного вектора будем стремиться получить вектор из нулей и одной 1, стоящей на месте слова контекста.\n",
    "\n",
    "В предположении, что перемножение матрицы и вектора происходит за линейное время от размера матрицы, сложность одной эпохи (проход по всей вики) составляет $O(W\\cdot d \\cdot T \\cdot c)$, где $c$ - количество слов в контексте данного, $T$ - количество способов выбрать слово контекста, $W\\cdot d$ - размер матрицы. Действительно, для каждого слова берется $c$ пар входных и выходных векторов, тогда за эпоху будет взято $T \\cdot c$ пар входных и выходных векторов, для каждого вектора выполняется перемножение за $W\\cdot d$. Стоит отметить, что для более точной оценки стоит добавить коэфф. примерно равный 4 (2 перемножения матриц, а затем обратных проход).\n",
    "\n",
    "Данная оценка не предусматривает одну вещь: сложность нахождения значения функции потерь. Однако, можно считать, что коэфф. $k$ в формуле функции потерь очень мал. Я не знаю, как в tensorflow устроена NCE, но для нее существует возможность вычисления за время, сильно меньшее $W$, поскольку нам нужно брать только некоторые значения выходного вектора. В любом случае, считая, что сигмоида считается за $O(1)$, $k << W$, время вычисления функции потерь сильно меньше времени перемножения матриц. Поэтому наша оценка времени не изменится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ну что же, время обучать модель. Поскольку такое ощущение, что keras написан на языке Кумир, у которого интерпретатор написан на Питоне, то пришлось использовать tensorflow.\n",
    "\n",
    "Для начала считаем наш словарик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy\n",
    "numpy.random.seed(1337)  # for reproducibility\n",
    "\n",
    "input = open('words-1.txt', 'r')\n",
    "word_dict = dict()\n",
    "s = input.readline()\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    num = int(s[1])\n",
    "    word_dict[s[0]] = num\n",
    "    s = input.readline()\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Теперь создадим модель. \n",
    "\n",
    "Я использовал размерность вектора $d=192$ для быстроты. Код почти полностью скопирован из базового примера tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import copy\n",
    "import math\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EMB_SIZE = 192\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([len(word_dict), EMB_SIZE], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([len(word_dict), EMB_SIZE],\n",
    "                                 stddev=1.0 / math.sqrt(EMB_SIZE)))\n",
    "        nce_biases = tf.Variable(tf.zeros([len(word_dict)]))\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                         biases=nce_biases,\n",
    "                         labels=train_labels,\n",
    "                         inputs=embed,\n",
    "                         num_sampled=64,\n",
    "                         num_classes=len(word_dict)))\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    # Add variable initializer.\n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Здесь я запускаю обучение. Выполняется 4 прохода по вики. Полученные матрицы сохраняются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    #print(\"Initialized\")\n",
    "    for loops in range(4):\n",
    "        input = open('simplewiki-parsed3.txt', 'r')\n",
    "        now_context = [-1 for i in range(9)]\n",
    "        now_pred = numpy.zeros((BATCH_SIZE, 1))\n",
    "        now_batch = numpy.zeros((BATCH_SIZE))\n",
    "        now_ind = 0\n",
    "        s = input.readline()\n",
    "        c = 0\n",
    "        prev_c = 0\n",
    "        CNT = 0\n",
    "        average_loss = 0\n",
    "        while len(s) > 0:\n",
    "            s = s.split()\n",
    "            if len(s) > 0 and s[0] == \"tttrrr\":\n",
    "                s = []\n",
    "            for elem in s:\n",
    "                for i in range(0, len(now_context) - 1):\n",
    "                    now_context[i] = now_context[i + 1]\n",
    "                now_context[-1] = word_dict[elem]\n",
    "                if now_context[0] != -1:\n",
    "                    for j in range(0, len(now_context)):\n",
    "                        if j != len(now_context) // 2:\n",
    "                            now_pred[now_ind, 0] = now_context[j]\n",
    "                            now_batch[now_ind] = now_context[len(now_context) // 2]\n",
    "                            now_ind += 1\n",
    "                            if now_ind == BATCH_SIZE:\n",
    "                                CNT += 1\n",
    "                                feed_dict = {train_inputs: now_batch, train_labels: now_pred}\n",
    "                                _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                                now_ind = 0\n",
    "                                average_loss += loss_val\n",
    "            s = input.readline()\n",
    "            c += 1\n",
    "            if c // 50000 > prev_c:\n",
    "                prev_c += 1\n",
    "                #print(prev_c, end = '')\n",
    "                average_loss /= 50000\n",
    "                #print(\": \", average_loss)\n",
    "                average_loss = 0\n",
    "        input.close()\n",
    "    final_emb = (embeddings / tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))).eval()\n",
    "    numpy.save('mas-Adagrad_norm', final_emb)\n",
    "    final_emb2 = embeddings.eval()\n",
    "    numpy.save('mas-Adagrad', final_emb2)\n",
    "    final_emb = nce_weights.eval()\n",
    "    numpy.save('__mas_weights', final_emb)\n",
    "    final_emb2 = nce_biases.eval()\n",
    "    numpy.save('__mas_biases', final_emb2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Мы сохранили наши матрицы, можно перейти к тестированию. Загружаем наш словарик и нашу матрицу представлений слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37810, 192)\n",
      "37810\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import copy\n",
    "\n",
    "mas = numpy.load('mas-Adagrad.npy')\n",
    "print(mas.shape)\n",
    "\n",
    "input = open('words-1.txt', 'r')\n",
    "word_dict = dict()\n",
    "s = input.readline()\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    num = int(s[1])\n",
    "    word_dict[s[0]] = num\n",
    "    s = input.readline()\n",
    "print(len(word_dict))\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Функция, которая по строке, представляющей из себя линейную комбинацию слов (см. пример), возвращает ближайшее слово. Фактически, это является расширенной версией функции $get\\_nearest$. Также здесь убираются из списка ближайших слов слова, которые входили в линейную комбинацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_nearest_linear_comb(s, n = 11, metric = \"cos\"):\n",
    "    string = s.split()\n",
    "    vect = mas[word_dict[string[0]]]\n",
    "    for i in range(1, len(string), 2):\n",
    "        vect2 = mas[word_dict[string[i + 1]]]\n",
    "        if string[i] == '-':\n",
    "            vect = vect - vect2\n",
    "        elif string[i] == '+':\n",
    "            vect = vect + vect2\n",
    "        else:\n",
    "            print('Error: wrong input string vector')\n",
    "    min_d = [10000000000000000000000000 for i in range(n)]\n",
    "    ans = ['' for i in range(len(min_d))]\n",
    "    for word in word_dict:\n",
    "        if metric == \"cos\":\n",
    "            d = 2 - get_cos_dist_v(vect, word)\n",
    "        else:\n",
    "            d = get_dist_v(vect, word)\n",
    "        if word in s:\n",
    "            d = 1000000000000000000000000000000\n",
    "        pst = False\n",
    "        for p in range(0, len(min_d)):\n",
    "            if not(pst) and d < min_d[p]:\n",
    "                for j in range(len(min_d) - 1, p, -1):\n",
    "                    min_d[j] = min_d[j - 1]\n",
    "                    ans[j] = ans[j - 1]\n",
    "                min_d[p] = d\n",
    "                ans[p] = word\n",
    "                pst = True\n",
    "    print(' '.join(string) + \": [\" + ', '.join(ans) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putin + country: [russia, vladimir, republics, alexander, sovereignty, government, saddam, tajikistan, demanding, belarus, resigning]\n"
     ]
    }
   ],
   "source": [
    "get_nearest_linear_comb('putin + country', metric=\"cos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Косинусная метрика работает достаточно адекватно, тогда как евклидова работает довольно плохо. Одно из ее свойств: для подавляющего большинства слов она выдает слово %UNKNOWN в списке ближайших. Также в списке ближайших часто можно заметить \"мусорные\" слова: остатки разметки вики и т.д. Например, для слова 'cat' в евклидовой метрике ближайшие слова выглядят так:\n",
    "\n",
    "[cats, dog, %UNKNOWN, bird, category, etc, face, pink, ref, local, family]\n",
    "\n",
    "Видно, что тут помимо %UNKNOWN влезло минимум 3 мусорных слова: \"etc\", \"ref\", \"category\". Последние 2 из них, скорее всего, остались от разметки википедии.\n",
    "\n",
    "По этой причине я приведу список близких слов только для косинусной меры.\n",
    "\n",
    "putin: [vladimir, hosni, resigning, suleiman, ballmer, medvedev, andrei, stalin's, preparations, mikhail, khomeini] (очень странно на самом деле, список совсем другой, нежели в LSA)\n",
    "\n",
    "moscow: [russian, petersburg, russia, soviet, sergei, ukrainian, oblast, ussr, kiev, belarus, berlin] (а вот тут список лучше, чем в LSA)\n",
    "\n",
    "lord: [baron, temple, lords, god, king, earl, jesus, duke, sir, henry, christ]  (ну тут тоже чуть лучше, чем в LSA, как мне кажется)\n",
    "\n",
    "computer: [rubbish, device, computing, logic, laptop, computers, hardware, graphics, electronics, mathematics, griffinofwales]    (rubbish, лол)\n",
    "\n",
    "siberia: [russia, pacific, alaska, mongolia, europe, greenland, america, oceans, iceland, ussr, mountains] (ну, хоть не tripoli)\n",
    "\n",
    "morning: [afternoon, evening, night, saturday, herald, day, sunday, late, friday, arrived, yesterday]\n",
    "\n",
    "moon: [earth, mars, planet, jupiter, uranus, apollo, neptune, lunar, moons, spacecraft, sun]\n",
    "\n",
    "nuclear: [atomic, reactor, chemical, weapons, energy, fusion, conservation, physics, power, radiation, solar]   (тут тоже лучше получилось)\n",
    "\n",
    "ship: [ships, boat, sail, vessel, sank, sinks, sailing, navy, sunk, warship, sinking]    (и тут лучше)\n",
    "\n",
    "job: [jobs, work, responsibility, idea, advice, skills, working, experience, training, care, him]  (гораздо лучше)\n",
    "\n",
    "wikipedia: [welcome, changing, your, changes, english, simple, to, pages, thank, enwiki, project]\n",
    "\n",
    "escape: [kill, escaped, enter, steal, survive, destroy, rescue, flee, catch, deliver, recover]\n",
    "\n",
    "outpost: [regimes, uprisings, vicinity, deserted, plateaus, looting, outcast, casinos, sey, scarborough, systematically]\n",
    "  (вот здесь похуже)\n",
    "\n",
    "car: [truck, bicycle, cars, automobile, driver, vehicle, motor, boat, racing, trucks, accident]   (а здесь получше)\n",
    "\n",
    "team: [teams, league, championship, football, tournament, hockey, soccer, club, player, cup, play]\n",
    "\n",
    "hobbit: [lego, penguin, shrek, monkey, chaucer, spider-man, middle-earth, pagos, sparrow, rogen, mcgowan]  (лего-хоббит, являющийся пингвином-обезьяной, который человек-паук в Средиземье :-) Ну хоть Средиземье есть)\n",
    "\n",
    "forest: [forests, park, hills, pine, trees, mountains, tree, mountain, wildlife, reserve, habitat]\n",
    "\n",
    "dragon: [monkey, pok, hulk, sword, panda, flower, tattoo, mon, kid, anime, boy]    (А-А-А-А!! Дракон-обезьяна-халк-панда-АНИМЕ)\n",
    "\n",
    "imperial: [royal, navy, military, orders, marshal, japanese, empire, tokyo, emperor, naval, air]\n",
    "\n",
    "metric: [measurement, limit, units, measures, measure, notation, measurements, max, newtons, yield, thus]\n",
    "\n",
    "empty: [specified, parameter, otherwise, blank, caption, namespace, image, inside, string, invalid, itself]\n",
    "\n",
    "dinosaurs: [mammals, reptiles, jurassic, cretaceous, birds, extinction, mesozoic, fossils, evolution, insects, creatures]\n",
    "  (и при чем тут млекопитающие?..)\n",
    "\n",
    "actor: [actress, comedian, screenwriter, writer, singer, musician, film, composer, director, songwriter, television]  (чуть хуже, мне кажется)\n",
    "\n",
    "light: [beam, waves, speed, sunlight, ultraviolet, bulb, sun, heat, shade, radiation, dust]\n",
    "\n",
    "jedi: [clone, sith, dooku, ninja, obi-wan, kenobi, phantom, skywalker, darth, master, mutant]\n",
    "\n",
    "september: [october, november, july, august, december, june, april, february, march, january, 11]    (ГОРАЗДО лучше)\n",
    "\n",
    "star: [trek, planet, episode, trilogy, clone, stars, wars, sun, galaxy, dwarf, heroes]\n",
    "\n",
    "nice: [interesting, okay, glad, fun, awesome, wonderful, ok, silly, big, btw, we'll]\n",
    "\n",
    "language: [languages, dialect, words, word, alphabet, spoken, arabic, literature, writing, basic, speak]\n",
    "\n",
    "street: [avenue, streets, wall, theatre, mall, corner, broadway, hotel, bus, square, 1929]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Алгоритм определения столицы:\n",
    "moscow - russia + [country]. Пример: moscow - russia + poland: warsaw\n",
    "\n",
    "#### Алгоритм нахождения множественного числа существительного:\n",
    "\n",
    "[word] + countries - country. Примеры: \n",
    "\n",
    "city: [town, borough, city's, county, capital, district, metropolitan, province, river, mayor, state], но\n",
    "\n",
    "city + countries - country: [cities, people, paris, town, areas, towns, 2011, borough, cases, geneva, river];\n",
    "\n",
    "man: [woman, person, boy, man's, mask, men, soldier, girl, him, himself, guy], но\n",
    "\n",
    "man + countries - country: [people, men, humans, woman, beings, camps, iron, victims, nations, societies, creatures]\n",
    "\n",
    "Здесь важно сказать, почему именно country-countries. Дело в том, что у многих существительных единственное и множественное число лежат очень рядом. А у слова country среди близких слов нет countries. Аналогично были взяты слова для примеров: у слов man и city их формы множественного числа лежат не ближе всего, поэтому на их примере видно, что алгоритм работает. Впрочем, он также работает для слов, у которых множественное число лежит рядом с единственным.\n",
    "\n",
    "#### Алгоритм нахождения превосходной степени прилагательного:\n",
    "\n",
    "[word] + oldest - old. Примеры:\n",
    "\n",
    "rich: [poor, z, young, fat, clay, dangerous, expensive, friends, loving, man, sugar], но\n",
    "\n",
    "rich + oldest - old: [largest, world's, **richest**, smallest, longest, biggest, most, highest, earliest, important, strongest];\n",
    "\n",
    "early: [late, beginning, mid, 19th, 18th, 1960s, earliest, 1940s, began, period, 17th], но\n",
    "\n",
    "early + oldest - old: [largest, **earliest**, biggest, longest, world's, smallest, surviving, best-known, first, richest, late].\n",
    "\n",
    "Тут видно, что возникают некоторые проблемы: например, упорно вылезают слова largest, biggest, smallest. Однако, это, видимо, связано со свойствами вики. Я пытался сделать более сложные комбинации для нахождения вектора превосходной степени (например, oldest - old + largest - large + [word]), но это только ухудшало ситуацию.\n",
    "\n",
    "#### Алгоритм нахождения антонима:\n",
    "\n",
    "[word] - elder + younger. Пример:\n",
    "\n",
    "simple: [english, write, wiktionary, basic, me, writing, something, wikipedia, vocabulary, guidelines, paragraph], но\n",
    "\n",
    "simple - elder + younger: [english, understand, me, administrators, **difficult**, simpler, articles, wiktionary, regular, write, **complex**]\n",
    "\n",
    "На самом деле антонимы создавать фактически невозможно, поскольку очень часто антоним - одно из ближайших слов к данному слову. Это означает, что вектор разницы очень мал, что приводит к трудностям.\n",
    "\n",
    "#### Алгоритм нахождения генерального директора компании:\n",
    "\n",
    "Отсутствует. Ну то есть наверное это должно быть что-то типа [company] + zuckerberg - facebook, но поскольку ближайшее слово к zuckerberg - это koala; ближайшее к слову gates - это doors; слов yandex, vkontakte (vk) вообще нет в списке, как и владельцев/основателей google, то вариантов крайне мало, и ни один из них не работает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cначала загрузим словарик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.random.seed(1337)  # for reproducibility\n",
    "\n",
    "input = open('words-1.txt', 'r')\n",
    "word_dict = dict()\n",
    "s = input.readline()\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    num = int(s[1])\n",
    "    word_dict[s[0]] = num\n",
    "    s = input.readline()\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Создадим нашу сеть. (Я почти дословно переписал код из примера реализации на github.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import copy\n",
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EMB_SIZE = 192\n",
    "COUNT_MAX = 100\n",
    "SCAL_FACTOR = 0.75\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    cooccurrence_count = tf.placeholder(tf.float32, shape=[BATCH_SIZE])\n",
    "    count_max = tf.constant([COUNT_MAX], dtype=tf.float32)\n",
    "    scal_factor = tf.constant([SCAL_FACTOR], dtype=tf.float32)\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_uniform([len(word_dict), EMB_SIZE], 1.0, -1.0))\n",
    "    contexts = tf.Variable(tf.random_uniform([len(word_dict), EMB_SIZE], 1.0, -1.0))\n",
    "    focal_biases = tf.Variable(tf.random_uniform([len(word_dict)], 1.0, -1.0))\n",
    "    context_biases = tf.Variable(tf.random_uniform([len(word_dict)], 1.0, -1.0))\n",
    "\n",
    "    focal_embedding = tf.nn.embedding_lookup([embeddings], train_inputs)\n",
    "    context_embedding = tf.nn.embedding_lookup([contexts], train_labels)\n",
    "    focal_bias = tf.nn.embedding_lookup([focal_biases], train_inputs)\n",
    "    context_bias = tf.nn.embedding_lookup([context_biases], train_labels)\n",
    "    \n",
    "    magic_function = tf.minimum(1.0, tf.pow(tf.div(cooccurrence_count, count_max), scal_factor))\n",
    "    embedding_product = tf.reduce_sum(tf.mul(focal_embedding, context_embedding), 1) \n",
    "    log_cooccurrences = tf.log(tf.to_float(cooccurrence_count))\n",
    "    distance_expr = tf.square(tf.add_n([embedding_product, focal_bias, context_bias, tf.neg(log_cooccurrences)]))\n",
    "\n",
    "    single_losses = tf.mul(magic_function, distance_expr)\n",
    "    total_loss = tf.reduce_sum(single_losses)\n",
    "    optimizer = tf.train.AdagradOptimizer(0.1).minimize(total_loss)\n",
    "        \n",
    "    combined_embeddings = tf.add(embeddings, contexts)\n",
    "    \n",
    "    init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Создадим нужную нам матрицу встречаемости пар слов и запишем в файлы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33756626\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 9\n",
    "now_context = ['' for i in range(WINDOW_SIZE)]\n",
    "cooccurences_dict = dict()\n",
    "input = open('simplewiki-parsed3.txt', 'r')\n",
    "s = input.readline()\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    if s[0] == 'tttrrr':\n",
    "        s = []\n",
    "    for elem in s:\n",
    "        for j in range(0, len(now_context) - 1):\n",
    "            now_context[j] = now_context[j+1]\n",
    "        now_context[-1] = elem\n",
    "        if now_context[0] != '':\n",
    "            for j in range(len(now_context)):\n",
    "                if j != len(now_context) // 2:\n",
    "                    cur_word = word_dict[now_context[len(now_context) // 2]]\n",
    "                    context_word = word_dict[now_context[j]]\n",
    "                    if not((cur_word, context_word) in cooccurences_dict):\n",
    "                        cooccurences_dict[(cur_word, context_word)] = 0\n",
    "                    cooccurences_dict[(cur_word, context_word)] += 1\n",
    "    s = input.readline()\n",
    "print(len(cooccurences_dict))\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output = open('context_words.txt', 'w')\n",
    "for elem in cooccurences_dict:\n",
    "    output.write(str(elem[0]) + ' ' + str(elem[1]) + ' ' + str(cooccurences_dict[elem]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В реализации на github предлагалось выкидывать редкие пары слов. Спойлеры: на simplewiki результат оказался лучше, если ничего не выкидывать. Однако, я все же создал матрицы с выкинутыми редкими парами слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output = open('context_words-8.txt', 'w')\n",
    "for elem in cooccurences_dict:\n",
    "    if cooccurences_dict[elem] >= 8:\n",
    "        output.write(str(elem[0]) + ' ' + str(elem[1]) + ' ' + str(cooccurences_dict[elem]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output = open('context_words-4.txt', 'w')\n",
    "for elem in cooccurences_dict:\n",
    "    if cooccurences_dict[elem] >= 4:\n",
    "        output.write(str(elem[0]) + ' ' + str(elem[1]) + ' ' + str(cooccurences_dict[elem]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del cooccurences_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Теперь из файла считаем нашу матрицу и распихаем по батчам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "batches = list()\n",
    "input = open('context_words.txt', 'r')\n",
    "c = 0\n",
    "prev_c = 0\n",
    "num_batch = 0\n",
    "while True:\n",
    "    counts = [0 for _ in range(BATCH_SIZE)]\n",
    "    i_s = [0 for _ in range(BATCH_SIZE)]\n",
    "    j_s = [0 for _ in range(BATCH_SIZE)]\n",
    "    is_good = True\n",
    "    for i in range(BATCH_SIZE):\n",
    "        s = input.readline()\n",
    "        if not(len(s) > 0):\n",
    "            is_good = False\n",
    "            break\n",
    "        s = s.split()\n",
    "        ind = num_batch * BATCH_SIZE + i\n",
    "        counts[i] = int(s[2])\n",
    "        i_s[i] = int(s[0])\n",
    "        j_s[i] = int(s[1])\n",
    "        c += 1\n",
    "        if c // 756626 > prev_c:\n",
    "            prev_c += 1\n",
    "            print(prev_c)\n",
    "    if not(is_good):\n",
    "        break\n",
    "    batches.append((i_s, j_s, counts))\n",
    "    num_batch += 1\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33756544\n"
     ]
    }
   ],
   "source": [
    "print(len(batches) * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Запустим обучение и запишем результаты в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "    for loops in range(15):\n",
    "        shuffle(batches)\n",
    "        num_batch = 0\n",
    "        average_loss = 0\n",
    "        c = 0\n",
    "        prev_c = 0\n",
    "        while num_batch < len(batches):\n",
    "            feed_dict = {train_inputs: batches[num_batch][0],\n",
    "                         train_labels: batches[num_batch][1],\n",
    "                         cooccurrence_count: batches[num_batch][2]}\n",
    "            num_batch += 1\n",
    "            _, loss_val = session.run([optimizer, total_loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "            c += 1\n",
    "            if c // 55000 > prev_c:\n",
    "                prev_c += 1\n",
    "                #print(loops, prev_c, end = '')\n",
    "                average_loss /= 55000\n",
    "                #print(\": \", average_loss)\n",
    "                average_loss = 0\n",
    "    final_embeddings = combined_embeddings.eval()\n",
    "    numpy.save('mas-GloVe-3', final_embeddings)\n",
    "    final_embeddings = embeddings.eval()\n",
    "    numpy.save('mas-GloVe-emb-3', final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Загрузим результаты и протестируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37810, 192)\n",
      "37810\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "mas = numpy.load('mas-GloVe-3.npy')\n",
    "print(mas.shape)\n",
    "\n",
    "input = open('words-1.txt', 'r')\n",
    "word_dict = dict()\n",
    "s = input.readline()\n",
    "while len(s) > 0:\n",
    "    s = s.split()\n",
    "    num = int(s[1])\n",
    "    word_dict[s[0]] = num\n",
    "    s = input.readline()\n",
    "print(len(word_dict))\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman: [son, ii, iii, becomes, charles, henry, wife, father, queen, daughter, iv]\n"
     ]
    }
   ],
   "source": [
    "get_nearest_linear_comb('king - man + woman', metric=\"cos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "На самом деле несложно заметить, что GloVe работает гораздо нестабильнее word2vec. В принципе, причины этого интуитивно понятны, и скорее всего, на больших объемах информации GloVe все же лучше работает. С редкими же словами типа jedi тут вообще все плохо. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь с помощью t-SNE сожмем размерности векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "for i in range(7):\n",
    "    tsne = TSNE(n_components=3, init='pca', n_iter=2000)\n",
    "    low_dim_words = tsne.fit_transform(mas[i * 5000:(1+i) * 5000])\n",
    "    numpy.save('3-dim-' + str(i), low_dim_words)\n",
    "    print(i)\n",
    "tsne = TSNE(n_components=3, init='pca', n_iter=2000)\n",
    "low_dim_words = tsne.fit_transform(mas[35000:])\n",
    "numpy.save('3-dim-7', low_dim_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим теперь это в массив mas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "(15000, 3)\n",
      "(20000, 3)\n",
      "(25000, 3)\n",
      "(30000, 3)\n",
      "(35000, 3)\n",
      "(37810, 3)\n"
     ]
    }
   ],
   "source": [
    "mas = numpy.load('3-dim-0.npy')\n",
    "for i in range(1, 8):\n",
    "    mas_dim_tmp = numpy.load('3-dim-' + str(i) + '.npy')\n",
    "    mas = numpy.vstack((mas, mas_dim_tmp))\n",
    "    print(mas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " king: [ 7.33472172  6.69764573  2.12835252]; queen: [ 8.31488053  7.62103707  2.1785307 ]\n",
      " man: [ 9.9640069   7.09351232  3.52569622]; woman: [ 9.89715238  7.08593285  3.39410687]\n",
      " boy: [ 8.66733077  5.0023829   3.12086132]; girl :[ 10.21238821  -3.61633332   0.35204872]\n",
      " big: [ 14.990779     4.22304282   1.2110358 ]; bigger :[ 12.27757342  -3.44382059  -5.37788218]; biggest: [ 3.16754041  7.86934359 -8.62689548]\n",
      " old: [ 13.58951928   6.82853117  -0.20036769]; older :[ 8.81268815  6.61042942  0.95334649]; oldest: [ 2.63830026  7.69876419 -9.05797181]\n",
      " small: [ 9.20078563  6.49464465  4.37043929]; smaller: [ 11.91363545  -3.34732108  -5.44103887]; smallest: [ 9.3489626  -7.93435585 -1.39372896]\n"
     ]
    }
   ],
   "source": [
    "print(' king: ', mas[word_dict['king']], '; queen: ', mas[word_dict['queen']],\n",
    "      '\\n man: ', mas[word_dict['man']], '; woman: ', mas[word_dict['woman']],\n",
    "      '\\n boy: ', mas[word_dict['boy']], '; girl :', mas[word_dict['girl']], sep = '')\n",
    "print(' big: ', mas[word_dict['big']], '; bigger :', mas[word_dict['bigger']], \n",
    "      '; biggest: ', mas[word_dict['biggest']],\n",
    "      '\\n old: ', mas[word_dict['old']], '; older :', mas[word_dict['older']], \n",
    "      '; oldest: ', mas[word_dict['oldest']], \n",
    "      '\\n small: ', mas[word_dict['small']], '; smaller: ', mas[word_dict['smaller']], \n",
    "      '; smallest: ', mas[word_dict['smallest']], sep = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что слова man, king, woman, boy, queen смотрят примерно в одном и том же направлении, как и old, older. Однако, со словами big, bigger, biggest, small, smaller, smallest очевидно, имеются какие-то проблемы. Аналогичную вещь можно сказать про girl.\n",
    "\n",
    "Кстати, bigger, smaller направлены одинаково."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь касательно семплирования нулей матрицы встречаемости пар в GloVe. Внимательно посмотрев на формулу ошибки и статью авторов GloVe, можно увидеть, что добавление пар векторов с нулевой координатой в матрице в батч (увеличивая размер батча) вообще не повлияет на ошибку и на градиент. Это означает, что добавление в батч таких векторов никак не влияет на обучаемость. То есть батч размера $n+m$, в котором $n$ пар векторов с нулевой матричной координатой, равносилен батчу размера $m$. Таким образом, семплировать нули не имеется смысла. Более того, семплирование нулей очень замедлит обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте сравним модели на основе поиска аналогий. Для этого я выкачал и распарсил специальную таблицу. Каждая строка таблицы состоит из 4 слов $w_1, w_2, w_3, w_4$. Предполагается, что $w_4 = w_2 - w_1 + w_3$. На основе этого определяется, насколько способ превращения слов в векторы хорош. А именно, я смотрю ближайшие 10 слов к данной комбинации, выкидывая слова, входящие в нее. Если $w_4$ лежит в этом списке, то тест считается пройденным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13989\n"
     ]
    }
   ],
   "source": [
    "words = list()\n",
    "answers = list()\n",
    "input = open('test_words_parsed.txt', 'r')\n",
    "s = input.readline().split()\n",
    "while len(s) == 4:\n",
    "    words.append((s[0], s[1], s[2]))\n",
    "    answers.append(s[3])\n",
    "    s = input.readline().split()\n",
    "input.close()\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, возвращающая список ближайших к вектору слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_nearest(s, n = 10, metric = 'cos'):\n",
    "    string = s.split()\n",
    "    vect = mas[word_dict[string[0]]]\n",
    "    for i in range(1, len(string), 2):\n",
    "        vect2 = mas[word_dict[string[i + 1]]]\n",
    "        if string[i] == '-':\n",
    "            vect = vect - vect2\n",
    "        elif string[i] == '+':\n",
    "            vect = vect + vect2\n",
    "        else:\n",
    "            print('Error: wrong input string vector')\n",
    "    min_d = [10000000000000000000000000 for i in range(n)]\n",
    "    ans = ['' for i in range(len(min_d))]\n",
    "    for word in word_dict:\n",
    "        if metric == \"cos\":\n",
    "            d = 2 - get_cos_dist_v(vect, word)\n",
    "        else:\n",
    "            d = get_dist_v(vect, word)\n",
    "        if word in s:\n",
    "            d = 1000000000000000000000000000000\n",
    "        pst = False\n",
    "        for p in range(0, len(min_d)):\n",
    "            if not(pst) and d < min_d[p]:\n",
    "                for j in range(len(min_d) - 1, p, -1):\n",
    "                    min_d[j] = min_d[j - 1]\n",
    "                    ans[j] = ans[j - 1]\n",
    "                min_d[p] = d\n",
    "                ans[p] = word\n",
    "                pst = True\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой ячейке я по очереди загружал в память представления слов и тестировал:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4%\n",
      "8%\n",
      "12%\n",
      "16%\n",
      "20%\n",
      "24%\n",
      "28%\n",
      "32%\n",
      "36%\n",
      "40%\n",
      "44%\n",
      "48%\n",
      "52%\n",
      "56%\n",
      "60%\n",
      "64%\n",
      "68%\n",
      "72%\n",
      "76%\n",
      "80%\n",
      "84%\n",
      "88%\n",
      "92%\n",
      "96%\n",
      "0.2808635356351419\n"
     ]
    }
   ],
   "source": [
    "mas = numpy.load('mas-GloVe-3.npy')\n",
    "count_true = 0\n",
    "c = 0\n",
    "prev_c = 0\n",
    "for i in range(len(answers)):\n",
    "    nearest = return_nearest(words[i][1] + ' - ' + words[i][0] + ' + ' + words[i][2], n=10)\n",
    "    if answers[i] in nearest:\n",
    "        count_true += 1\n",
    "    c += 1\n",
    "    if prev_c < c // 560:\n",
    "        prev_c += 1\n",
    "        print(prev_c * 4, '%', sep = '')\n",
    "print(count_true / len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты\n",
    "**LSA** - 0.10529701908642505\n",
    "\n",
    "**Skip-Gram** - 0.4973193223246837\n",
    "\n",
    "**GloVe** - 0.2808635356351419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## И-и-и в нашем соревновании побеждает...\n",
    "\n",
    "# ...Skip-Gram!!! \n",
    "\n",
    "## *(громкие аплодисменты в зале, смущенный Skip-Gram выходит на сцену, играет торжественный гимн, одна из девушек кричит \"я хочу от тебя эмбеддинги\", GloVe в слезах выбегает из зала)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
